# Landing Page A/B Testing

The following is taken from _Julian.com's_ [Growth Marketing Guide](https://www.julian.com/guide/growth/ab-testing).

## What is A/B Testing

A/B Testing is a way to continuously improve conversion rates on your website. These are experiments that assess the improvement in conversion rate after making changes to your site. 

When you make a change to your site, it is a called a variant. The "A" is the baseline: what your site was like before, and the "B" is the change that you're making, e.g. changing copy, reducing page length, adding illustrations. 

Free tools exist to do this for you. Your job is to figure out what is worth testing and to create the landing page material. 

## A/B Testing is a Must

A/B testing is **the only scientific way to improve conversion**. 

Don't waste time making your first iteration perfect. Make it good enough (8/10), and then A/B test it to perfection. You will never have a perfect page to begin and it is irrational to try. 

A/B Tests cost nothing to run and often increase conversion by up to 300%. A/B testing only takes a few days of work (after which there are severely diminishing returns), but can often be the difference between paid user acquisition being profitable. 

In other words, it makes or breaks growth. 

Very few people understand A/B testing. 

## How A/B Testing works

In summary: 
> 1. Decide what change to make. What do you think might increase conversion?
> 2. Use Google Optimize (an A/B testing tool) to show half your visitors the change.
> 3. Run this test long enough to get a statistically significant sample of visitors.
> 4. Once enough data has been collected, Google Optimize will report the likelihood that your changes caused a significant difference in conversion. If it caused a significant positive difference, it's up to you to implement in the winning variant's changes into the existing page.
> 5. Log what you changed, why you made the change, and what the results were. This log helps you avoid conducting overly similar experiments in the future.
> 6. Repeat steps 1-5 until you run out of ideas. Never have downtime; every day of the year, an A/B test should be running. Or you're letting your traffic go to waste.

## How to source A/B Test ideas

Use the following places to come up with ideas: 

> 1. Support and sales teams — Team members who interact with your customers know best what appeals to them. Ask what objections they commonly see, then proactively address those objections in your landing page copy.
> 2. User surveys — Poll users for their favorite features and biggest concerns. Weave these into your landing page copy.
> 3. Best ads — Your best performing ads have value props, text, and imagery that can be repurposed for your site. In fact, you should run ads for the explicit purpose of finding your most enticing text and images. (I cover this on an upcoming ads page.)
> 4. Competitors' sites — Identify successful competitors in your space and mine their pages for inspiration. Do they structure their content differently? Do they talk to visitors differently? Test mimicking their style.
> 5. On-site behavior — Use a visitor recording tool like Hotjar or FullStory to find patterns in visitors' engagement: What are they clicking? What are they ignoring? And what does this imply about the type of content that most appeals to them? Test giving your visitors more of it.
> 6. Past A/B successes and failures — I soon cover how to report your A/B successes and failures. Be sure to revisit them for lessons that inform your new tests.

## A/B Testing and the growth funnel

An A/B variant is only better when it increases your revenue. 
More revenue is better than more signups. 

It's easy to test a variant that decreases total signups (weeding out visitors who weren't willing to pay) while increasing revenue by better incentivising those who were unsure.

The success metric of an A/B test is therefore whether it does or does not increase revenue: i.e end-of-funnel conversion. 

**_However_**, you'll be doing most of your testing earlier in the funnel because:

1. **Bigger samples:** There's more traffic at the start of the funnel. This means you have bigger sample sizes, which means getting test results sooner.
2. **Less implementation:** It's easier to make changes to your landing page than your product. 

## What to A/B test on your Landing Page

Two types of A/B variants: micro and macro. 

Micro adjustments: copy, layout, creative -- small and quick.
Macro adjustments: significant rethinking of your page. Takes priority. 

Macro adjustments can increase conversion by between 50%-300% if your original page hasn't been tested before. Micro adjustments (e.g colour changes) are never more than 5%. 

Every A/B test has an opportunity cost: there are only so many you can run in a month. So prioritise the big wins. 

### Micro variants

Here's a list of things that constitute micro variants:

> 1. Text — Header, subheader, feature headers, feature paragraphs
> 2. Imagery — Header image, content images, background images
CTA — CTA button design, placement, copy
> 3. Social proof — Try different company logos or different forms of proof
> 4. Forms — Number of fields, field layout, and field copy
> 5. Order — The order of your page sections
> 6. Design — Spacing, color, and font styling
> 7. Offers — Introduce time-sensitive discounts

Lots of micros at once make a macro. 

### High impact micros 

You should prioritise the following micros:

> 1.**Completely rewriting header and subheader copy** — Header text is the first hook into your product. So, if you've been unknowingly showing visitors unenticing text here, fixing it has an impact. (I cover header copy on the previous page.)
> 2. **Reordering above-the-fold content** — Every page has an "above-the-fold" (ATF) section. This is what visitors see before scrolling down to reveal the rest of a page. The content placed ATF determines whether visitors continue scrolling to learn everything they need to know to purchase. So, micro-test content changes in your ATF section.
> 3. **Completely rewriting intro paragraphs on blog posts** — You can A/B test your blog posts too! I do that with Julian.com: I write two entirely different intros for a page then test which gets people to read more.

### Macro variants: generating ideas

Macros are hard, but are essential to improve your site's design. The biggest obstacle to macro testing is committing them. Solution: create a calendar and stick to it. 

> 1. **Mimic sections of competitors' pages** — Find competitors with thoughtful, well-structured pages. Then mimic some of their page sections. By "sections," I refer not to their words, but to their layout tactics, such as charts, sliders, GIFs, and other means of displaying information. (Do not rip off their site. Only take inspiration.)
> 2. **Write to a new persona** — Tailor your value props and copy to, say, mothers instead of teenagers. Perhaps you’re misidentifying your most valuable audience.
> 3. **Cut the page in half** — Counterintuitively, having less content sometimes means more content is ultimately read. Because visitors are less overwhelmed with information. Try removing everything that isn't critical. Be hyper concise.
Take a stance — Choose one value prop that you embrace more than competitors. Write a page drawing that line in the sand; pick a narrow-minded fight and call out all the competitors who differ from your one, true way. Show visitors they're either with you or against you, and how being with you leads to a better outcome. For example, you're either a smart subscriber to a meal delivery service or you're the type of person who wastes their time and money on restaurants.
> 4. **Combine micros** — Combine a half-dozen micros that work toward a singular goal, such as reinforcing a value prop or compelling visitors to take a specific action.

## How to prioritise A/B Tests

Use the following criteria to know which A/Bs to try first: 

> 1. **Confidence** — How confident are you the test will succeed? You can sanity check confidence by better understanding users: survey them, monitor their on-site behavior, and study your past A/B's. That said, sometimes crazy ideas perform best.
> 2. **Impact** — If a tests succeeds, is it likely to significantly increase conversion? The less optimized your landing page is to start — or the more macro your proposed test is — the greater its potential impact. Higher impact tests should be run first.
> 3. **Implementation** — How easy it is to implement? Would the test's implementation exhaust team resources or introduce too much technical complexity? If so, deprioritize the test if you have other strong ideas that require less implementation. (When it is time to run a high-implementation variant, determine its minimum viable form that'll take roughly 20% of the time to build but prove 50% or more of the conversion potential. If that succeeds, then take the time to fully implement it.)
> 4. **Uniqueness** — Is your new test a near copy of previous one that failed? For example, are you changing the color of a button further down the page after a previous button color change higher on the page failed? (Granted, perhaps the page has changed enough since that first test that this new button color test is worth doing... How confident are you that's the case?)
> 5. **Brand consistency** — If adding aggressive sales copy increases your signup conversion, but you're a company that normally plays it safe, then perhaps going off-brand for better conversion is not a good tradeoff. Sometimes, it's wiser to care more about building a brand you're proud of than increasing your bottom line. If you're unsure whether an A/B compromises your brand, poll other team members.

## A/B Testing as a Way of Life

Four outcomes for any decision:

1. Succeeding and learning
2. Succeeding without learning
3. Failure and learning
4. Failure without learning

Failure without learning **is a waste of time**. 

> Plan your big decisions in such a way that failure will teach you something new and profound about how to make better future decisions. That way — even in abject failure — you can never truly lose.

## Setting Up A/B Tests

### How many at once?

One experiment at a time. You can have several variants all testing a change on the same baseline page. Each variant receives the same amount of proportioned site traffic as the rest (handled by Google Optimize). 

More than one and it's difficult to track users across simultaneous tests, rendering them useless. 

### Parallel vs sequential

A/B tools test variants in parallel. Your original page and its variants run at the same time, and the tool sends visitors to one or the other. 

If you do it sequentially, i.e. the baseline for 5 days then the variant for 5 days, you don't contorl for changes in traffic sources or days of the week. Stick to parallel.

### Deciding who to exclude

You can exclude certain kinds of visitors from seeing your tests. Consider only showing your tests to new users, otherwise not everyone will arrive at your site with the same knowledge, which may affect how they react to your variant. 

### Assessing A/B test results

> 1. At minimum, you tested on your lowest-intent audience.
> 2. You've achieved sufficient sample size.
> 3. You're focusing on end-of-funnel performance.
> 4. You're writing down hypotheses for why your test succeeded or failed. So your insights can inform future tests. Increasing revenue is just one benefit of A/B tests. Another is increasing your efficiency at coming up with great tests.

### Low-intent vs high-intent traffic

A/B tests produce quickly diminishing returns on high-intent traffic: organic search, referrals, word of mouth. They came looking for you, so all you need to do is affirm you do what they're expecting. 

For paid ad traffic, A/B testing has greater potential to improve conversion. These are people who are looking for excuses to leave your site. 

> When I run A/B tests on paid traffic, I can often improve conversion rates by 2-4x. That can make or break the profitability of ads. It's a big deal. However, when I A/B test with organic traffic, perhaps I see 1.5-2x improvements at best. (Assuming the landing page was good to begin with.)
>
> Here’s the takeaway: If you only A/B against high-intent traffic, you may not notice a significant improvement and may mistakenly dismiss the test as a failure. When this happens, but you're confident the variant has potential, retry the test on paid traffic. That’s where the improvement may be large enough to notice its significance.

## Sample Sizes

> To statistically validate a 6.3%+ conversion increase, a test needs 1,000+ visits.
> To statistically validate a 2%+ conversion increase, a test needs 10,000+ visits.

If you have low traffic, macro variants are better, as they often result in 10-20% improvements. The anayltics tools of Google Optimize has a _Probability to be Best_ column. If the variant's probability exceeds 70% and has a sufficient number of sessions (1000+ or 10,000+) the results are statistically sound and should be considered for implementation. Is the labour worth the potential gain? You have to decide. 

### Sample sizes and revenue

> The closer an experiment's conversion objective is tied to revenue, the more worthwhile it is to patiently await small conversion boosts. 

If you've 1,000  page visits, and a 2% conversion increase for, say, newsletter signups, maybe it's not worth waiting for the 10,000 in order to validate your results. **However**, if your experiment is tracking conversions to paying customers, i.e. increased revenue, then it may be worth waiting for the complete results. 

Don't implement negligible wins (relative to the business outcomes you care about). 

## Tracking A/B Tests

For every test, track: 

> 1. **Conversion event** — What I'm optimizing for, such as clicks, views, or time on site.
> 2. **Before and after** — I include screenshots and descriptions of what's being tested.
> 3. **Reasoning** — I explain why this test is worth running. I refer to the earlier prioritization criteria: confidence, impact, implementation, uniqueness, and brand.

When the test finishes: 

> 1. **Start and end dates** — Products change over time, so have a timeline anchor.
> 2. **Results** — The change in conversion, and whether the result was neutral, a success, or a failure. (I rely on  Optimize to determine confidence intervals here.) If it was a success, I note whether the variant was then actually implemented into the site.
> 3. **Sample size** — As reported by the A/B testing tool. Let's hope it was big!
> 4. **Discussion** — 1) What, if anything, can be learned from the result? 2) Also, were there confounding issues (e.g. a big, strange traffic source that wasn't excluded from the test audience) that could have dirtied the results?

## Summary

> All marketers know about A/B testing, but few do it. Because it requires reinventing the wheel and team collaboration. But you need to be doing it. It's higher-leverage and cheaper than most marketing initiatives.
> 
> Focus on macro variants until you run out of ideas. When resorting to micro variants, focus on those that directly impact revenue (e.g. purchases) instead of conversion objectives earlier in your funnel (e.g. signups).
> 
> Diligently keep track of A/B results and reference them when ideating future tests.